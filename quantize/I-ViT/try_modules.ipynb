{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78a4405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master_112/m56121041/miniconda3/envs/aoc_final/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from utils import *  \n",
    "import hook \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module, Parameter\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581dc78b",
   "metadata": {},
   "source": [
    "## Test QuantAct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd61780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.randn(1, 768).to(device)\n",
    "quant_act = QuantAct(activation_bit=8, quant_mode='symmetric').to(device)\n",
    "output, scale = quant_act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "877fde5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[ 0.0552, -0.3588, -0.9661,  2.8983,  1.2697, -1.1869, -0.3588, -0.6901,\n",
      "         -0.4140,  1.5457, -0.5244, -0.9937,  0.0276, -0.5520, -1.2421, -0.8005,\n",
      "         -0.9661,  0.9385,  0.8281,  0.4140,  0.8557,  0.9109, -0.6349,  0.1104,\n",
      "         -1.3801,  0.4968,  1.2697, -0.6901, -1.1869,  0.4140, -0.5797, -0.2208,\n",
      "          0.6349, -0.0276, -0.2760,  0.5520,  2.3186, -0.0276, -0.9385, -0.4968,\n",
      "          1.4629, -0.3312, -2.2082, -0.2484, -0.1656,  0.4968,  0.6625,  0.1932,\n",
      "         -0.5520, -0.6349, -1.1041, -0.3312,  0.6901, -0.5520,  0.2484,  0.1656,\n",
      "          0.0276, -0.5244,  1.7114, -0.2760,  1.1317, -0.7177, -1.9322,  0.7453,\n",
      "         -1.9322, -0.5244,  1.6009,  0.3864,  0.0552, -0.2484, -1.1041, -0.0000,\n",
      "         -1.2145,  1.0213, -1.0765, -0.4416,  0.7453, -1.5457,  1.6285,  1.6561,\n",
      "         -0.1656,  1.0213,  0.8005,  0.3864,  1.1869, -0.0276, -0.2208, -0.1932,\n",
      "          0.3864,  0.0000,  1.1593,  0.1656, -1.6285, -1.0213, -1.0489, -0.6073,\n",
      "          0.2484, -0.9937, -2.2082,  0.2484, -1.4629,  0.8557,  1.7942, -0.6349,\n",
      "          1.6009, -0.6349,  0.0552,  1.4905,  0.3312, -0.8281, -0.0828,  0.4140,\n",
      "         -0.1380,  0.2208, -1.2973, -0.6349,  0.1932,  0.5520,  0.1932, -0.4416,\n",
      "         -0.3312,  0.2484,  1.9598, -0.0000, -0.8833, -0.0552, -1.7114, -1.3801,\n",
      "          1.5457,  1.0213,  0.5244,  0.7177,  0.5520, -1.8218, -1.1869, -0.3588,\n",
      "         -0.5520,  0.2208, -1.7666,  1.2421,  0.3036,  0.7729, -1.3801,  0.5520,\n",
      "          1.6009, -1.4077, -0.5520, -0.4416,  0.6073, -2.3186,  1.7666, -0.3588,\n",
      "         -0.7177, -1.4353,  0.8833, -0.3036, -0.2484, -1.0489,  0.4692,  0.8833,\n",
      "         -0.7729,  0.1380, -2.3738, -0.4692, -0.0552, -0.4968, -0.3588, -0.6349,\n",
      "          0.0000,  0.3588, -0.4692, -1.1041, -0.8005,  0.3312, -0.5244,  0.0000,\n",
      "          0.4968,  1.2697, -0.3864,  1.9322,  0.2208, -0.0552,  1.2973,  0.5244,\n",
      "         -0.4140,  1.1317, -0.0828,  0.0276, -1.0489, -0.3588, -0.2208,  0.0828,\n",
      "         -1.6009, -0.1104, -0.9661,  0.2208, -0.8281,  0.3864,  0.8281, -0.9109,\n",
      "          0.8833, -1.6561,  0.9937, -0.3588,  0.0276,  1.3525, -0.9937, -0.3312,\n",
      "          1.8218,  1.7390,  0.1656, -0.8281,  1.3249, -0.9661,  0.1104, -0.8833,\n",
      "          0.4692,  0.2484,  1.1869,  0.3312,  0.3588, -0.2484, -0.2208,  0.7729,\n",
      "          0.4692, -0.3864,  0.0552, -0.1656,  0.8005, -0.7177, -0.2484, -0.2208,\n",
      "         -2.0702, -0.7177, -0.8557,  0.6901,  1.2697, -0.1656, -0.8833,  0.8557,\n",
      "          0.0000,  1.2973, -1.7942, -0.1380, -0.1380,  0.1656, -0.1656,  0.3312,\n",
      "          0.0828,  1.3249, -1.6009, -1.7390, -1.0213,  0.7453, -0.3312,  0.1656,\n",
      "         -0.9385, -0.1380,  1.1869, -1.3801, -0.4416,  1.2421, -0.7729, -0.1380,\n",
      "          0.7729,  0.0000, -0.4692, -0.0276, -1.1869,  0.8281, -0.6073, -0.0552,\n",
      "         -2.9811,  0.6901, -1.1041,  0.0276, -0.7729, -1.5181, -0.0000, -0.0828,\n",
      "         -0.5244,  0.5797, -0.8281, -0.9661, -0.7453,  1.6009,  0.3588,  0.5520,\n",
      "          0.6625,  0.5244,  0.5797,  1.1317,  0.1656,  0.4140,  0.2760, -0.1656,\n",
      "          0.0552, -0.4416, -0.5797, -1.1869,  1.2421, -0.9109, -1.8218, -0.9109,\n",
      "         -0.3588, -0.2760, -0.7729,  0.4968,  1.1041,  1.1869, -0.8557, -1.1869,\n",
      "         -0.1104, -1.7114, -1.3801, -0.2208,  0.0828,  1.2697,  1.5181, -0.5797,\n",
      "         -0.9385, -0.7729,  1.1041, -0.1104, -0.5244,  0.7729, -0.7453, -0.1932,\n",
      "         -0.8557,  0.0552, -0.1656, -0.1656,  0.0552, -1.0489,  0.1104, -0.7729,\n",
      "          2.3186,  1.2973,  0.6901, -0.1104,  0.8005,  0.0000, -1.1041,  0.9385,\n",
      "         -1.1317, -1.2697, -0.9385,  0.0552,  0.3588, -0.0828, -0.2208, -0.6625,\n",
      "          0.1104, -1.7114,  1.7390, -0.7729, -0.5797,  2.5118,  0.2208, -0.2208,\n",
      "          0.6349,  0.6073, -0.3588,  1.1317,  0.1656, -0.1380,  0.8005,  0.1380,\n",
      "         -1.0213, -0.5797,  0.1104,  0.1932, -0.0276, -2.1530,  0.1656,  0.9109,\n",
      "         -0.5797, -1.1041,  0.5244, -0.4968,  0.1104,  1.2145,  0.4692,  0.1656,\n",
      "          0.8281, -0.0276, -1.5457,  0.7453, -0.7453,  1.0765, -0.0276,  2.2082,\n",
      "          0.1932,  0.2484, -0.8833,  0.1104,  1.3249, -1.0213, -0.1932, -1.4353,\n",
      "          0.2760,  0.2484,  0.5244,  0.4968, -0.2484, -2.3186,  0.6073, -0.5520,\n",
      "         -1.1593, -1.6561, -1.6009, -1.9322,  0.3036,  1.1869,  0.9385,  0.8281,\n",
      "          0.2208,  0.8557,  0.5244,  1.1869, -0.0276,  0.5244, -0.7177, -0.5244,\n",
      "          0.1656, -0.4692,  0.8557,  0.9937,  1.6285, -0.4140, -1.3249,  3.0639,\n",
      "         -1.7114, -1.1593,  2.0978, -0.1932, -0.9385,  0.6349, -1.2421, -1.6561,\n",
      "          1.3249, -0.1932, -0.3036, -0.8557, -0.3312,  0.2208, -0.7177,  0.8281,\n",
      "         -1.7390,  0.5797,  0.4140,  2.2634, -0.6349,  1.3801, -1.9046,  1.4905,\n",
      "          1.5457,  0.7453,  1.0489,  0.6901,  0.6625,  0.0276,  0.8833,  0.4968,\n",
      "          0.1656,  1.0213,  0.5520, -1.2421,  0.7177,  0.3864,  1.6009, -0.8557,\n",
      "          0.5797,  1.0489, -0.1104, -0.5244,  0.0828, -2.5394, -1.5457,  0.4968,\n",
      "         -1.1593, -1.9322, -0.2760, -0.6349,  1.0489,  1.7114,  1.4905,  0.3588,\n",
      "          0.3864,  0.9109,  0.1656,  1.2973, -1.0213,  0.7177,  1.1317, -0.9109,\n",
      "          0.5520,  1.7942,  0.0828, -0.1380,  0.5244, -1.0765, -1.1317, -1.4353,\n",
      "         -1.3801, -0.1656, -0.1380, -0.3864,  0.4968,  0.1104, -1.4077,  0.8005,\n",
      "         -1.0765, -1.2697,  0.3312,  1.1593,  1.2697,  0.0828,  1.2973, -0.4416,\n",
      "         -0.5520, -1.6009,  0.4968, -0.1380, -0.8833,  0.6901,  0.8557,  0.6349,\n",
      "          0.3588, -0.8005,  0.8005,  0.0000,  0.8833,  0.1104,  0.8005, -0.8281,\n",
      "         -3.2571, -0.0552,  0.2760, -0.2208, -0.0828,  1.6285, -0.1932, -0.6073,\n",
      "          1.6838,  0.5520,  0.7177,  0.4416,  0.0276,  2.3738, -1.0765,  0.3864,\n",
      "         -0.1380,  2.0426, -0.7453,  0.4416, -0.4692,  0.3312, -1.6838, -0.1104,\n",
      "         -1.4353,  1.5733, -0.5520, -0.5797,  0.9109,  0.1656,  0.1656,  0.2760,\n",
      "          0.1932,  1.9322, -1.6838, -0.6349,  0.2760,  1.1869, -1.4353,  1.0489,\n",
      "         -1.7666, -0.7729,  0.1104, -0.7453, -0.7729,  0.7453, -0.3864, -2.2358,\n",
      "          0.9109, -0.2760, -1.9598, -0.5520, -1.6561,  0.2760, -1.9874, -0.7177,\n",
      "          0.0000,  0.3312, -0.9661,  0.0828, -0.6073, -0.1380,  0.0552,  0.2484,\n",
      "         -0.1104,  0.0276,  0.1656, -1.0765, -0.2484,  1.5457, -1.6285, -0.1104,\n",
      "         -0.3588, -1.3525, -1.0489,  0.1104, -0.0552, -0.1380,  0.4692, -0.7453,\n",
      "          1.8770, -0.9937, -0.8557,  0.6073, -0.9937, -0.2484,  0.1104, -0.9385,\n",
      "         -0.3036, -0.2760,  1.6838,  0.6625, -1.3249, -0.0828,  1.7666, -0.9385,\n",
      "          1.8494,  2.0702, -0.9937,  0.0828,  0.2760,  0.0000,  0.0276,  0.9661,\n",
      "          0.9109, -0.5520, -0.0000,  0.5244, -0.4968, -0.3588,  0.4140, -0.7177,\n",
      "          0.2484, -1.1041,  1.1593, -1.1041, -2.4014, -0.8557,  0.1656,  0.7453,\n",
      "          0.6349, -0.1932, -1.0489, -0.3864,  0.9937,  1.2421, -0.9385,  0.3312,\n",
      "          0.1104,  0.2484,  0.9109, -0.1932,  0.1932, -1.1593,  0.3588, -0.1656,\n",
      "          2.0702,  0.1656,  0.0828,  3.5055, -0.2208,  0.2208,  0.3864,  0.0828,\n",
      "          0.9109, -1.2973,  0.6349, -1.1317, -1.0213, -0.6073,  1.3249,  1.4353,\n",
      "          0.9661, -1.0213, -0.7453,  0.1656, -0.3036, -2.1530,  0.4692, -0.3588,\n",
      "          0.7177, -0.1104,  0.1932, -0.7177,  1.0213,  0.0552,  0.4140,  0.6349,\n",
      "         -0.1932, -1.4353,  0.8557, -0.2760, -0.1656, -0.8833, -0.1380,  1.0765,\n",
      "          1.1317,  1.0489,  3.2571,  1.3249,  0.6073, -0.2484,  0.6625,  0.9109,\n",
      "          1.0489,  0.5244, -0.6073, -0.1104, -0.0000, -0.2208,  0.0552,  0.5244,\n",
      "          1.1869, -0.7729,  0.6349, -0.4692,  0.3312,  0.6349,  0.4692, -0.5520,\n",
      "          0.0276,  0.3588, -0.5797, -0.5244,  0.6901, -0.9937, -0.4968, -0.8833,\n",
      "         -0.3588, -0.4140,  0.5520,  2.5118,  1.9322,  0.8557, -0.0552,  0.4140,\n",
      "         -0.5520, -1.9598,  0.1656, -0.5520, -1.0213, -0.5797, -0.2760,  0.4692,\n",
      "         -0.8281, -0.4140, -0.4416,  0.2484,  0.1380,  0.2208, -0.0828, -0.3864]],\n",
      "       device='cuda:0')\n",
      "Scale: tensor(0.0276, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\", output)\n",
    "print(\"Scale:\", scale)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a4e7ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovered INT: tensor([[   2.,  -13.,  -35.,  105.,   46.,  -43.,  -13.,  -25.,  -15.,   56.,\n",
      "          -19.,  -36.,    1.,  -20.,  -45.,  -29.,  -35.,   34.,   30.,   15.,\n",
      "           31.,   33.,  -23.,    4.,  -50.,   18.,   46.,  -25.,  -43.,   15.,\n",
      "          -21.,   -8.,   23.,   -1.,  -10.,   20.,   84.,   -1.,  -34.,  -18.,\n",
      "           53.,  -12.,  -80.,   -9.,   -6.,   18.,   24.,    7.,  -20.,  -23.,\n",
      "          -40.,  -12.,   25.,  -20.,    9.,    6.,    1.,  -19.,   62.,  -10.,\n",
      "           41.,  -26.,  -70.,   27.,  -70.,  -19.,   58.,   14.,    2.,   -9.,\n",
      "          -40.,   -0.,  -44.,   37.,  -39.,  -16.,   27.,  -56.,   59.,   60.,\n",
      "           -6.,   37.,   29.,   14.,   43.,   -1.,   -8.,   -7.,   14.,    0.,\n",
      "           42.,    6.,  -59.,  -37.,  -38.,  -22.,    9.,  -36.,  -80.,    9.,\n",
      "          -53.,   31.,   65.,  -23.,   58.,  -23.,    2.,   54.,   12.,  -30.,\n",
      "           -3.,   15.,   -5.,    8.,  -47.,  -23.,    7.,   20.,    7.,  -16.,\n",
      "          -12.,    9.,   71.,   -0.,  -32.,   -2.,  -62.,  -50.,   56.,   37.,\n",
      "           19.,   26.,   20.,  -66.,  -43.,  -13.,  -20.,    8.,  -64.,   45.,\n",
      "           11.,   28.,  -50.,   20.,   58.,  -51.,  -20.,  -16.,   22.,  -84.,\n",
      "           64.,  -13.,  -26.,  -52.,   32.,  -11.,   -9.,  -38.,   17.,   32.,\n",
      "          -28.,    5.,  -86.,  -17.,   -2.,  -18.,  -13.,  -23.,    0.,   13.,\n",
      "          -17.,  -40.,  -29.,   12.,  -19.,    0.,   18.,   46.,  -14.,   70.,\n",
      "            8.,   -2.,   47.,   19.,  -15.,   41.,   -3.,    1.,  -38.,  -13.,\n",
      "           -8.,    3.,  -58.,   -4.,  -35.,    8.,  -30.,   14.,   30.,  -33.,\n",
      "           32.,  -60.,   36.,  -13.,    1.,   49.,  -36.,  -12.,   66.,   63.,\n",
      "            6.,  -30.,   48.,  -35.,    4.,  -32.,   17.,    9.,   43.,   12.,\n",
      "           13.,   -9.,   -8.,   28.,   17.,  -14.,    2.,   -6.,   29.,  -26.,\n",
      "           -9.,   -8.,  -75.,  -26.,  -31.,   25.,   46.,   -6.,  -32.,   31.,\n",
      "            0.,   47.,  -65.,   -5.,   -5.,    6.,   -6.,   12.,    3.,   48.,\n",
      "          -58.,  -63.,  -37.,   27.,  -12.,    6.,  -34.,   -5.,   43.,  -50.,\n",
      "          -16.,   45.,  -28.,   -5.,   28.,    0.,  -17.,   -1.,  -43.,   30.,\n",
      "          -22.,   -2., -108.,   25.,  -40.,    1.,  -28.,  -55.,   -0.,   -3.,\n",
      "          -19.,   21.,  -30.,  -35.,  -27.,   58.,   13.,   20.,   24.,   19.,\n",
      "           21.,   41.,    6.,   15.,   10.,   -6.,    2.,  -16.,  -21.,  -43.,\n",
      "           45.,  -33.,  -66.,  -33.,  -13.,  -10.,  -28.,   18.,   40.,   43.,\n",
      "          -31.,  -43.,   -4.,  -62.,  -50.,   -8.,    3.,   46.,   55.,  -21.,\n",
      "          -34.,  -28.,   40.,   -4.,  -19.,   28.,  -27.,   -7.,  -31.,    2.,\n",
      "           -6.,   -6.,    2.,  -38.,    4.,  -28.,   84.,   47.,   25.,   -4.,\n",
      "           29.,    0.,  -40.,   34.,  -41.,  -46.,  -34.,    2.,   13.,   -3.,\n",
      "           -8.,  -24.,    4.,  -62.,   63.,  -28.,  -21.,   91.,    8.,   -8.,\n",
      "           23.,   22.,  -13.,   41.,    6.,   -5.,   29.,    5.,  -37.,  -21.,\n",
      "            4.,    7.,   -1.,  -78.,    6.,   33.,  -21.,  -40.,   19.,  -18.,\n",
      "            4.,   44.,   17.,    6.,   30.,   -1.,  -56.,   27.,  -27.,   39.,\n",
      "           -1.,   80.,    7.,    9.,  -32.,    4.,   48.,  -37.,   -7.,  -52.,\n",
      "           10.,    9.,   19.,   18.,   -9.,  -84.,   22.,  -20.,  -42.,  -60.,\n",
      "          -58.,  -70.,   11.,   43.,   34.,   30.,    8.,   31.,   19.,   43.,\n",
      "           -1.,   19.,  -26.,  -19.,    6.,  -17.,   31.,   36.,   59.,  -15.,\n",
      "          -48.,  111.,  -62.,  -42.,   76.,   -7.,  -34.,   23.,  -45.,  -60.,\n",
      "           48.,   -7.,  -11.,  -31.,  -12.,    8.,  -26.,   30.,  -63.,   21.,\n",
      "           15.,   82.,  -23.,   50.,  -69.,   54.,   56.,   27.,   38.,   25.,\n",
      "           24.,    1.,   32.,   18.,    6.,   37.,   20.,  -45.,   26.,   14.,\n",
      "           58.,  -31.,   21.,   38.,   -4.,  -19.,    3.,  -92.,  -56.,   18.,\n",
      "          -42.,  -70.,  -10.,  -23.,   38.,   62.,   54.,   13.,   14.,   33.,\n",
      "            6.,   47.,  -37.,   26.,   41.,  -33.,   20.,   65.,    3.,   -5.,\n",
      "           19.,  -39.,  -41.,  -52.,  -50.,   -6.,   -5.,  -14.,   18.,    4.,\n",
      "          -51.,   29.,  -39.,  -46.,   12.,   42.,   46.,    3.,   47.,  -16.,\n",
      "          -20.,  -58.,   18.,   -5.,  -32.,   25.,   31.,   23.,   13.,  -29.,\n",
      "           29.,    0.,   32.,    4.,   29.,  -30., -118.,   -2.,   10.,   -8.,\n",
      "           -3.,   59.,   -7.,  -22.,   61.,   20.,   26.,   16.,    1.,   86.,\n",
      "          -39.,   14.,   -5.,   74.,  -27.,   16.,  -17.,   12.,  -61.,   -4.,\n",
      "          -52.,   57.,  -20.,  -21.,   33.,    6.,    6.,   10.,    7.,   70.,\n",
      "          -61.,  -23.,   10.,   43.,  -52.,   38.,  -64.,  -28.,    4.,  -27.,\n",
      "          -28.,   27.,  -14.,  -81.,   33.,  -10.,  -71.,  -20.,  -60.,   10.,\n",
      "          -72.,  -26.,    0.,   12.,  -35.,    3.,  -22.,   -5.,    2.,    9.,\n",
      "           -4.,    1.,    6.,  -39.,   -9.,   56.,  -59.,   -4.,  -13.,  -49.,\n",
      "          -38.,    4.,   -2.,   -5.,   17.,  -27.,   68.,  -36.,  -31.,   22.,\n",
      "          -36.,   -9.,    4.,  -34.,  -11.,  -10.,   61.,   24.,  -48.,   -3.,\n",
      "           64.,  -34.,   67.,   75.,  -36.,    3.,   10.,    0.,    1.,   35.,\n",
      "           33.,  -20.,   -0.,   19.,  -18.,  -13.,   15.,  -26.,    9.,  -40.,\n",
      "           42.,  -40.,  -87.,  -31.,    6.,   27.,   23.,   -7.,  -38.,  -14.,\n",
      "           36.,   45.,  -34.,   12.,    4.,    9.,   33.,   -7.,    7.,  -42.,\n",
      "           13.,   -6.,   75.,    6.,    3.,  127.,   -8.,    8.,   14.,    3.,\n",
      "           33.,  -47.,   23.,  -41.,  -37.,  -22.,   48.,   52.,   35.,  -37.,\n",
      "          -27.,    6.,  -11.,  -78.,   17.,  -13.,   26.,   -4.,    7.,  -26.,\n",
      "           37.,    2.,   15.,   23.,   -7.,  -52.,   31.,  -10.,   -6.,  -32.,\n",
      "           -5.,   39.,   41.,   38.,  118.,   48.,   22.,   -9.,   24.,   33.,\n",
      "           38.,   19.,  -22.,   -4.,   -0.,   -8.,    2.,   19.,   43.,  -28.,\n",
      "           23.,  -17.,   12.,   23.,   17.,  -20.,    1.,   13.,  -21.,  -19.,\n",
      "           25.,  -36.,  -18.,  -32.,  -13.,  -15.,   20.,   91.,   70.,   31.,\n",
      "           -2.,   15.,  -20.,  -71.,    6.,  -20.,  -37.,  -21.,  -10.,   17.,\n",
      "          -30.,  -15.,  -16.,    9.,    5.,    8.,   -3.,  -14.]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "recovered_int = (output / scale).round()\n",
    "print(\"Recovered INT:\", recovered_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e6b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c89af5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "output2 = recovered_int * scale\n",
    "print(torch.allclose(output, output2, atol=1e-5))  # True 表示量化 + 還原沒誤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f5c8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7a37c65",
   "metadata": {},
   "source": [
    "## IntGelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "485b14b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IntGELU(Module):\n",
    "    \"\"\"\n",
    "    Class to quantize given GELU layer\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    quant_mode : 'none' or 'symmetric', default 'none'\n",
    "        The mode for quantization. 'none' for no quantization.\n",
    "    force_dequant : str, default 'none'\n",
    "        Force dequantize GELU if either 'gelu' or 'nonlinear' is given.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 quant_mode='none',\n",
    "                 force_dequant='none'):\n",
    "        super(IntGELU, self).__init__()\n",
    "        self.register_buffer('input_scaling_factor', torch.ones(1))\n",
    "        self.quant_mode = quant_mode\n",
    "        if force_dequant in ['nonlinear', 'gelu']:\n",
    "            logger.info(\"Force dequantize gelu\")\n",
    "            self.quant_mode = 'none'\n",
    "\n",
    "\n",
    "        if self.quant_mode == 'none':\n",
    "            self.activation_fn = nn.GELU()\n",
    "        elif self.quant_mode == 'symmetric':\n",
    "            pass\n",
    "        elif quant_mode == \"asymmetric\":\n",
    "            raise NotImplementedError(\"unsupported quant mode: {}\".format(self.quant_mode))\n",
    "        else:\n",
    "            raise ValueError(\"unknown quant mode: {}\".format(quant_mode))\n",
    "\n",
    "        self.k = 1.4142\n",
    "        self.n = 14 # sufficiently large integer\n",
    "        self.coeff = [-0.2888, -1.769, 1] # a(x+b)**2 + c\n",
    "        self.coeff[2] /= self.coeff[0]\n",
    "\n",
    "    def fix(self):\n",
    "        pass\n",
    "\n",
    "    def unfix(self):\n",
    "        pass\n",
    "\n",
    "    def int_erf(self, x_int, scaling_factor):\n",
    "        with torch.no_grad():\n",
    "            b_int = torch.floor(self.coeff[1] / scaling_factor)\n",
    "            c_int = torch.floor(self.coeff[2] / scaling_factor ** 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sign = torch.sign(x_int)\n",
    "        abs_int = torch.abs(x_int)\n",
    "        abs_int = torch.min(abs_int, -b_int)\n",
    "        y_int = (abs_int + b_int) ** 2 + c_int\n",
    "        y_int = sign * y_int\n",
    "        scaling_factor = scaling_factor ** 2 * self.coeff[0]\n",
    "        y_int = floor_ste.apply(y_int / 2 ** self.n)\n",
    "        scaling_factor = scaling_factor * 2 ** self.n\n",
    "        \n",
    "        return y_int, scaling_factor\n",
    "\n",
    "    def forward(self, x, scaling_factor=None):\n",
    "        if self.quant_mode == 'none':\n",
    "            return self.activation_fn(x), None\n",
    "\n",
    "        assert self.quant_mode == 'symmetric', \\\n",
    "                \"unsupported quant mode: {}\".format(quant_mode)\n",
    "\n",
    "        x_int = x / scaling_factor\n",
    "        sigmoid_int, sigmoid_scaling_factor = self.int_erf(x_int, scaling_factor / self.k)\n",
    "\n",
    "        shift_int = torch.floor(1. / sigmoid_scaling_factor)\n",
    "\n",
    "        x_int = x_int * (sigmoid_int + shift_int)\n",
    "        scaling_factor = scaling_factor * sigmoid_scaling_factor / 2\n",
    "\n",
    "        return x_int * scaling_factor, scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f17aa82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IntGELU 測試 ===\n",
      "原始輸入 (float): tensor([-0.6101,  2.5926, -0.7107, -0.8551, -0.3036], device='cuda:0')\n",
      "浮點 GELU: tensor([-0.1653,  2.5803, -0.1696, -0.1678, -0.1156], device='cuda:0')\n",
      "整數近似 GELU: tensor([  1469.5619, -24884.5801,   1714.4888,   2057.3865,    734.7809],\n",
      "       device='cuda:0')\n",
      "MSE: 57368348.0\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Function\n",
    "# floor STE（讓整數化時的非可導操作能反向傳播）\n",
    "class floor_ste(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return torch.floor(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "# IntGELU 請假設你已經事先定義好\n",
    "\n",
    "def test_intgelu():\n",
    "    # 1. 選擇 device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 2. 建立浮點輸入資料\n",
    "    x_fp = torch.randn(1, 128).to(device)\n",
    "    act_bit = 8\n",
    "    qmax = 2 ** (act_bit - 1) - 1\n",
    "\n",
    "    # 3. 手動計算 scaling factor 並量化\n",
    "    max_val = x_fp.abs().max()\n",
    "    scale = (max_val / qmax).clamp(min=1e-8)\n",
    "    x_int = torch.clamp((x_fp / scale).round(), -qmax, qmax).to(torch.int32)\n",
    "\n",
    "    # 4. 建立 IntGELU 模型並移到 device\n",
    "    gelu = IntGELU(quant_mode='symmetric').to(device)\n",
    "\n",
    "    # 5. 執行 IntGELU 推論（注意要轉成 float 傳入）\n",
    "    out_intgelu, out_scale = gelu(x_int.float(), scale)\n",
    "\n",
    "    # 6. 參考真實 GELU 結果\n",
    "    gelu_ref = nn.GELU().to(device)\n",
    "    out_fp = gelu_ref(x_fp)\n",
    "\n",
    "    # 7. 將 intgelu 輸出還原為 float 做比較\n",
    "    out_approx = out_intgelu / out_scale\n",
    "\n",
    "    # 8. 印出測試結果\n",
    "    print(\"=== IntGELU 測試 ===\")\n",
    "    print(\"原始輸入 (float):\", x_fp[0, :5])\n",
    "    print(\"浮點 GELU:\", out_fp[0, :5])\n",
    "    print(\"整數近似 GELU:\", out_approx[0, :5])\n",
    "    print(\"MSE:\", torch.mean((out_fp - out_approx) ** 2).item())\n",
    "\n",
    "test_intgelu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2f0a4",
   "metadata": {},
   "source": [
    "## tryQLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f97204a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "tensor([[ 0.0839, -0.8119,  0.5391,  1.3295],\n",
      "        [-1.2138, -0.2571,  0.4485,  2.0543],\n",
      "        [-0.2641,  0.2417, -0.1684, -0.4287]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(3, 4).to(device)\n",
    "print(w.shape[0])\n",
    "print(w.shape[1])\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63df4c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "qact = QuantAct()\n",
    "x = torch.randn(2, 3).to(device)\n",
    "quant_linear_test = QuantLinear(3, 4, weight_bit=8, quant_mode='symmetric').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe28454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4880, -1.3811,  1.4525],\n",
      "        [ 0.9484,  1.3777, -1.1465]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ef13453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Output: tensor([[ 1.4880, -1.3826,  1.4529],\n",
      "        [ 0.9490,  1.3826, -1.1482]], device='cuda:0')\n",
      "Scale: tensor(0.0117, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x1, scale = qact(x)\n",
    "print(\"Quantized Output:\", x1)\n",
    "print(\"Scale:\", scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8ea6363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Activation: tensor([[ 127., -118.,  124.],\n",
      "        [  81.,  118.,  -98.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "quanted_act = x1 / scale\n",
    "print(\"Quantized Activation:\", quanted_act) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "231a55e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.6942e-05, 4.6940e-05, 3.4805e-05, 3.8446e-05], device='cuda:0')\n",
      "Weights: tensor([[-1.3432, -0.1147, -0.7765,  1.1408],\n",
      "        [-0.0747, -1.9044,  0.2433,  0.9682]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "Quantized Weights: tensor([[-28613.,  -2444., -22309.,  29672.],\n",
      "        [ -1591., -40570.,   6989.,  25184.]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w1, w_scale = quant_linear_test(x1, scale)\n",
    "print(w_scale)\n",
    "print(\"Weights:\", w1)\n",
    "quanted_w = w1 / w_scale\n",
    "print(\"Quantized Weights:\", quanted_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f3b19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "reshp = torch.randn(2, 3, 4).to(device)\n",
    "reshp = reshp.reshape(-1, reshp.shape[-1])\n",
    "print(reshp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83340828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vit_quant import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eac7880",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deit_tiny_patch16_224(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4081f38",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QuantAct' object has no attribute 'x_min'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aoc_final/lib/python3.10/site-packages/torch/nn/modules/module.py:2952\u001b[0m, in \u001b[0;36mModule.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2950\u001b[0m child_lines \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2951\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 2952\u001b[0m     mod_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2953\u001b[0m     mod_str \u001b[38;5;241m=\u001b[39m _addindent(mod_str, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   2954\u001b[0m     child_lines\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m mod_str)\n",
      "File \u001b[0;32m~/aoc_final/quantize/I-ViT/models/quantization_utils/quant_modules.py:153\u001b[0m, in \u001b[0;36mQuantAct.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m(activation_bit=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    151\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_mode: \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m, Act_min: \u001b[39m\u001b[38;5;132;01m{3:.2f}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    152\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAct_max: \u001b[39m\u001b[38;5;132;01m{4:.2f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_bit,\n\u001b[0;32m--> 153\u001b[0m                                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_mode, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_min\u001b[49m\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_max\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/aoc_final/lib/python3.10/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QuantAct' object has no attribute 'x_min'"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9dea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vit_quant import Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e43757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"checkpoint.pth.tar\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518994fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设 model 是你的完整 ViT 模型，\n",
    "# 并且你已经准备好一个输入张量 input_tensor: shape [B, N, C]\n",
    "# （比如 embedding 之后加上 cls_token、pos_embed，再 dropout 之后的结果）。\n",
    "# 这里我们直接用随机数模拟：\n",
    "B, N, C = 1, 198, 192\n",
    "input_tensor = torch.randn(B, N, C)\n",
    "\n",
    "# 1. 直接用隨機的 x_norm，不再通過 model.blocks[0].norm1\n",
    "x_norm = torch.randn(B, N, C)  # 隨機生成代替原本的 norm1 輸出\n",
    "\n",
    "# 2. 對 x_norm 做 symmetric quantization 計算 scaling factor\n",
    "# Symmetric quantization: α = max(abs(x)) / (2^(bit-1) - 1)\n",
    "# 假設用 8-bit quantization\n",
    "\n",
    "n_bits = 8\n",
    "max_val = 2**(n_bits-1) - 1  # 127 for 8-bit\n",
    "α_in = torch.max(torch.abs(x_norm), dim=-1, keepdim=True)[0] / max_val  # shape [B, N, 1]\n",
    "# 或者如果你想要 per-channel scaling (shape=[C])：\n",
    "# α_in = torch.max(torch.abs(x_norm.view(-1, C)), dim=0)[0] / max_val  # shape [C]\n",
    "\n",
    "# 使用 per-channel scaling factor\n",
    "α_in = torch.max(torch.abs(x_norm.view(-1, C)), dim=0)[0] / max_val  # shape [C]\n",
    "\n",
    "# 3. 量化浮點 x_norm 到整數域\n",
    "x_int = (x_norm / α_in).round()  # shape [B, N, C]\n",
    "\n",
    "# 4. 現在就有了量化後的輸入 activation x_int，可以照之前的流程\n",
    "# 拿到 proj 層的整數權重和偏置，然後在整數域裡做線性：\n",
    "Wq_int = checkpoint[\"blocks.0.attn.qkv.weight_integer\"][:C, :]  # Q 部分\n",
    "bq_int = checkpoint[\"blocks.0.attn.qkv.bias_integer\"][:C]\n",
    "q_int = F.linear(x_int, Wq_int, bq_int)  # [B, N, C]\n",
    "\n",
    "# 5. 恢復到浮點域\n",
    "scale_w_q = checkpoint[\"blocks.0.attn.qkv.fc_scaling_factor\"][:C]  # [C]\n",
    "α_out_q = scale_w_q * α_in\n",
    "q_fp = q_int * α_out_q\n",
    "\n",
    "# 保存各個階段的整數結果到 txt 文件\n",
    "import numpy as np\n",
    "\n",
    "# 1. 保存 q_proj 的 input activation (int8)\n",
    "x_int_np = x_int.detach().cpu().numpy().astype(np.int8)\n",
    "np.savetxt('q_proj_input_activation_int8.txt', x_int_np.reshape(-1, C), fmt='%d', delimiter=',')\n",
    "print(f\"Saved q_proj input activation (int8) shape: {x_int_np.shape}\")\n",
    "\n",
    "# 2. 保存 q_proj 的 weight (int)\n",
    "Wq_int_np = Wq_int.detach().cpu().numpy().astype(np.int32)\n",
    "np.savetxt('q_proj_weight_int.txt', Wq_int_np, fmt='%d', delimiter=',')\n",
    "print(f\"Saved q_proj weight (int) shape: {Wq_int_np.shape}\")\n",
    "\n",
    "# 3. 保存 q_proj 的 bias (int32)\n",
    "bq_int_np = bq_int.detach().cpu().numpy().astype(np.int32)\n",
    "np.savetxt('q_proj_bias_int32.txt', bq_int_np.reshape(1, -1), fmt='%d', delimiter=',')\n",
    "print(f\"Saved q_proj bias (int32) shape: {bq_int_np.shape}\")\n",
    "\n",
    "# 4. 保存 q_proj 的 output (int32) - 這是線性層的直接輸出\n",
    "q_int_np = q_int.detach().cpu().numpy().astype(np.int32)\n",
    "np.savetxt('q_proj_output_int32.txt', q_int_np.reshape(-1, C), fmt='%d', delimiter=',')\n",
    "print(f\"Saved q_proj output (int32) shape: {q_int_np.shape}\")\n",
    "\n",
    "# 5. 保存 scaling factors 供參考\n",
    "scaling_factors = {\n",
    "    'alpha_in': α_in.detach().cpu().numpy(),\n",
    "    'scale_w_q': scale_w_q.detach().cpu().numpy(),\n",
    "    'alpha_out_q': α_out_q.detach().cpu().numpy()\n",
    "}\n",
    "np.savetxt('scaling_factors.txt', np.column_stack([scaling_factors['alpha_in'], \n",
    "                                                  scaling_factors['scale_w_q'], \n",
    "                                                  scaling_factors['alpha_out_q']]), \n",
    "           fmt='%.8f', delimiter=',', \n",
    "           header='alpha_in,scale_w_q,alpha_out_q')\n",
    "\n",
    "print(\"x_norm:\", x_norm[0,0,:5])\n",
    "print(\"α_in (scaling factor):\", α_in[:5])\n",
    "print(\"x_int:\", x_int[0,0,:5])\n",
    "print(\"q_int:\", q_int[0,0,:5])\n",
    "print(\"q_fp:\", q_fp[0,0,:5])\n",
    "print(\"\\nAll integer tensors saved to txt files!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aoc_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
