import torch
import torch.nn.functional as F
import numpy as np
import math

def float_to_dyadic(value, max_bits=8):
    """
    å°‡æµ®é»æ•¸è½‰æ›ç‚º dyadic æ ¼å¼: a â‰ˆ b / 2^c
    å…¶ä¸­ b å’Œ c éƒ½æ˜¯ int8 ç¯„åœå…§çš„æ•´æ•¸
    
    Args:
        value: æµ®é»æ•¸å€¼
        max_bits: b å’Œ c çš„æœ€å¤§ä½æ•¸ (é»˜èª8ä½)
    
    Returns:
        tuple: (original_float, b, c, approximated_float)
    """
    if value == 0:
        return (0.0, 0, 0, 0.0)
    
    # æ‰¾åˆ°æœ€ä½³çš„ c å€¼
    max_val = 2**(max_bits-1) - 1  # 127 for int8
    min_val = -2**(max_bits-1)     # -128 for int8
    
    best_error = float('inf')
    best_b, best_c = 0, 0
    
    # å˜—è©¦ä¸åŒçš„ c å€¼
    for c in range(max_bits):  # c å¾ 0 åˆ° 7
        scale = 2**c
        b_float = value * scale
        
        # b å¿…é ˆåœ¨ int8 ç¯„åœå…§
        if min_val <= b_float <= max_val:
            b = int(round(b_float))
            approx_value = b / scale
            error = abs(value - approx_value)
            
            if error < best_error:
                best_error = error
                best_b, best_c = b, c
    
    # å¦‚æœæ²’æ‰¾åˆ°åˆé©çš„å€¼ï¼Œä½¿ç”¨æœ€æ¥è¿‘çš„
    if best_error == float('inf'):
        c = max_bits - 1
        scale = 2**c
        b = int(np.clip(round(value * scale), min_val, max_val))
        best_b, best_c = b, c
    
    approximated = best_b / (2**best_c)
    return (float(value), int(best_b), int(best_c), approximated)

def generate_residual_int32(shape, bit_width=16):
    """
    ç”Ÿæˆæ¨¡æ“¬çš„ residual æ•¸æ“š (16-bit ç¯„åœä½†ç”¨ int32 å­˜å„²)
    
    Args:
        shape: å¼µé‡å½¢ç‹€
        bit_width: ä½å¯¬ (é»˜èª16ä½)
    
    Returns:
        torch.Tensor: int32 å¼µé‡ï¼Œæ•¸å€¼ç¯„åœåœ¨ 16-bit å…§
    """
    max_val = 2**(bit_width-1) - 1  # 32767 for 16-bit
    min_val = -2**(bit_width-1)     # -32768 for 16-bit
    
    # ç”Ÿæˆ 16-bit ç¯„åœçš„éš¨æ©Ÿæ•´æ•¸ï¼Œç”¨ int32 å­˜å„²
    residual = torch.randint(min_val, max_val + 1, shape, dtype=torch.int32)
    return residual

def quantized_inference_with_residual(checkpoint=None):
    """
    å®Œæ•´çš„é‡åŒ–æ¨ç†å¯¦ç¾ï¼ŒåŒ…å« residual é€£æ¥
    """
    print("=== Quantized Inference with Residual Connection ===")
    
    # æ¨¡æ“¬åƒæ•¸
    B, N, C = 1, 198, 192
    
    # 1. ç”Ÿæˆéš¨æ©Ÿè¼¸å…¥ (FP32)
    print("\n=== Step 1: Generate Random Input (FP32) ===")
    input_fp32 = torch.randn(B, N, C, dtype=torch.float32)
    print(f"Input FP32 shape: {input_fp32.shape}")
    print(f"Input FP32 range: [{input_fp32.min():.6f}, {input_fp32.max():.6f}]")
    
    # 2. è¨ˆç®—é‡åŒ–ç¸®æ”¾å› å­ä¸¦è½‰æ›ç‚º dyadic æ ¼å¼ (Per-tensor Symmetric)
    print("\n=== Step 2: Quantization Scaling Factor (Per-tensor Symmetric) ===")
    n_bits = 8
    max_val = 2**(n_bits-1) - 1  # 127 for 8-bit
    
    # Per-tensor scaling factor (æ•´å€‹tensoråªæœ‰ä¸€å€‹scaling factor)
    alpha_float_scalar = torch.max(torch.abs(input_fp32)) / max_val
    
    # è½‰æ›ç‚º dyadic æ ¼å¼
    orig_float, b, c, approx_float = float_to_dyadic(alpha_float_scalar.item())
    dyadic_results = [(orig_float, b, c, approx_float)]
    alpha_dyadic_scalar = torch.tensor(approx_float, dtype=torch.float32)
    
    print(f"Per-tensor scaling factor:")
    print(f"  Original: {orig_float:.8f}")
    print(f"  Dyadic: {b}/2^{c} = {approx_float:.8f}")
    print(f"  Error: {abs(orig_float-approx_float):.8f}")
    print(f"  b (int8): {b}, c (int8): {c}")
    
    # 3. è¼¸å…¥é‡åŒ– (ä½¿ç”¨ per-tensor dyadic scaling factor)
    print("\n=== Step 3: Input Quantization (Per-tensor Symmetric INT8) ===")
    input_quantized = (input_fp32 / alpha_dyadic_scalar).round().clamp(-max_val, max_val)
    input_int8 = input_quantized.to(torch.int8)
    print(f"Quantized input range: [{input_int8.min()}, {input_int8.max()}]")
    print(f"Per-tensor scaling applied: {alpha_dyadic_scalar:.8f}")
    
    # 4. ç”Ÿæˆé‡åŒ–æ¬Šé‡å’Œåç½®
    print("\n=== Step 4: Generate Quantized Weights and Bias ===")
    if checkpoint:
        # å¾ checkpoint åŠ è¼‰
        weight_int8 = checkpoint["blocks.0.attn.proj.weight_integer"].to(torch.int8)
        bias_int32 = checkpoint["blocks.0.attn.proj.bias_integer"].to(torch.int32)
        weight_scaling = checkpoint["blocks.0.attn.proj.fc_scaling_factor"]
    else:
        # ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š
        weight_int8 = torch.randint(-127, 128, (C, C), dtype=torch.int8)
        bias_int32 = torch.randint(-1000, 1000, (C,), dtype=torch.int32)
        weight_scaling = torch.rand(C) * 0.01
    
    print(f"Weight INT8 shape: {weight_int8.shape}")
    print(f"Bias INT32 shape: {bias_int32.shape}")
    print(f"Weight range: [{weight_int8.min()}, {weight_int8.max()}]")
    print(f"Bias range: [{bias_int32.min()}, {bias_int32.max()}]")
    
    # 5. ç·šæ€§é‹ç®— (INT32 è¼¸å‡º)
    print("\n=== Step 5: Linear Operation (INT32) ===")
    linear_output_int32 = F.linear(
        input_int8.float(), 
        weight_int8.float(), 
        bias_int32.float()
    ).to(torch.int32)
    
    print(f"Linear output INT32 shape: {linear_output_int32.shape}")
    print(f"Linear output range: [{linear_output_int32.min()}, {linear_output_int32.max()}]")
    
    # 6. ç”Ÿæˆ Residual (INT32, 16-bit ç¯„åœ)
    print("\n=== Step 6: Generate Residual (INT32, 16-bit range) ===")
    residual_int32 = generate_residual_int32((B, N, C), bit_width=16)
    print(f"Residual INT32 shape: {residual_int32.shape}")
    print(f"Residual range: [{residual_int32.min()}, {residual_int32.max()}]")
    
    # 7. Residual åŠ æ³• (INT32)
    print("\n=== Step 7: Residual Addition (INT32) ===")
    final_output_int32 = linear_output_int32 + residual_int32
    print(f"Final output INT32 shape: {final_output_int32.shape}")
    print(f"Final output range: [{final_output_int32.min()}, {final_output_int32.max()}]")
    
    # 8. ä¿å­˜æ‰€æœ‰çµæœç‚º TXT æ–‡ä»¶
    print("\n=== Step 8: Save All Results to TXT Files ===")
    save_all_results_to_txt(
        input_fp32, alpha_float_scalar, dyadic_results, alpha_dyadic_scalar,
        input_int8, weight_int8, bias_int32, linear_output_int32, 
        residual_int32, final_output_int32
    )
    
    # 9. é©—è­‰çµæœ
    print("\n=== Step 9: Verification ===")
    print(f"Input FP32 sample: {input_fp32[0, 0, :3]}")
    print(f"Alpha dyadic (per-tensor): {alpha_dyadic_scalar}")
    print(f"Input INT8 sample: {input_int8[0, 0, :3]}")
    print(f"Linear output sample: {linear_output_int32[0, 0, :3]}")
    print(f"Residual sample: {residual_int32[0, 0, :3]}")
    print(f"Final output sample: {final_output_int32[0, 0, :3]}")
    
    return {
        'input_fp32': input_fp32,
        'scaling_factor_float': alpha_float_scalar,
        'scaling_factor_dyadic': alpha_dyadic_scalar,
        'dyadic_parameters': dyadic_results,
        'input_int8': input_int8,
        'weight_int8': weight_int8,
        'bias_int32': bias_int32,
        'linear_output_int32': linear_output_int32,
        'residual_int32': residual_int32,
        'final_output_int32': final_output_int32
    }

def save_all_results_to_txt(input_fp32, alpha_float_scalar, dyadic_results, alpha_dyadic_scalar,
                           input_int8, weight_int8, bias_int32, linear_output_int32, 
                           residual_int32, final_output_int32):
    """
    ä¿å­˜æ‰€æœ‰çµæœåˆ° TXT æ–‡ä»¶ï¼Œä¾›ç¡¬é«” testbench ä½¿ç”¨
    """
    print("Saving all results to TXT files...")
    
    B, N, C = input_fp32.shape
    
    # 1. è¼¸å…¥ FP32 (198x192)
    input_fp32_np = input_fp32.squeeze(0).detach().cpu().numpy()  # Remove batch dimension
    np.savetxt('input_fp32.txt', input_fp32_np, fmt='%.8f', delimiter=',')
    print(f"âœ“ Saved input_fp32.txt: shape {input_fp32_np.shape}")
    
    # 2. Scaling factor (per-tensor, åªæœ‰ä¸€å€‹å€¼)
    orig_float, b, c, approx_float = dyadic_results[0]  # per-tensor åªæœ‰ä¸€çµ„å€¼
    scaling_data = np.array([[orig_float, b, c, approx_float]])
    
    np.savetxt('scaling_factor_per_tensor.txt', scaling_data, 
               fmt=['%.8f', '%d', '%d', '%.8f'], delimiter=',',
               header='original_float,b_int8,c_int8,approximated_float')
    print(f"âœ“ Saved scaling_factor_per_tensor.txt: per-tensor scaling factor")
    
    # 3. æ¬Šé‡ INT8 (192x192)
    weight_int8_np = weight_int8.detach().cpu().numpy()
    np.savetxt('weight_int8.txt', weight_int8_np, fmt='%d', delimiter=',')
    print(f"âœ“ Saved weight_int8.txt: shape {weight_int8_np.shape}")
    
    # 4. åç½® INT32 (192,)
    bias_int32_np = bias_int32.detach().cpu().numpy()
    np.savetxt('bias_int32.txt', bias_int32_np.reshape(1, -1), fmt='%d', delimiter=',')
    print(f"âœ“ Saved bias_int32.txt: shape {bias_int32_np.shape}")
    
    # 5. é‡åŒ–è¼¸å…¥ INT8 (198x192)
    input_int8_np = input_int8.squeeze(0).detach().cpu().numpy()
    np.savetxt('input_quantized_int8.txt', input_int8_np, fmt='%d', delimiter=',')
    print(f"âœ“ Saved input_quantized_int8.txt: shape {input_int8_np.shape}")
    
    # 6. ç·šæ€§é‹ç®—çµæœ INT32 (198x192)
    linear_output_np = linear_output_int32.squeeze(0).detach().cpu().numpy()
    np.savetxt('linear_output_int32.txt', linear_output_np, fmt='%d', delimiter=',')
    print(f"âœ“ Saved linear_output_int32.txt: shape {linear_output_np.shape}")
    
    # 7. Residual INT32 (198x192)
    residual_np = residual_int32.squeeze(0).detach().cpu().numpy()
    np.savetxt('residual_int32.txt', residual_np, fmt='%d', delimiter=',')
    print(f"âœ“ Saved residual_int32.txt: shape {residual_np.shape}")
    
    # 8. æœ€çµ‚çµæœ INT32 (198x192)
    final_output_np = final_output_int32.squeeze(0).detach().cpu().numpy()
    np.savetxt('final_output_int32.txt', final_output_np, fmt='%d', delimiter=',')
    print(f"âœ“ Saved final_output_int32.txt: shape {final_output_np.shape}")
    
    # 9. å‰µå»º README æ–‡ä»¶
    readme_content = """# Quantized Inference Test Data (Per-tensor Symmetric)

This directory contains test data for quantized inference with residual connection.
**Activation quantization: Per-tensor Symmetric**

## File Descriptions:

1. **input_fp32.txt** (198x192)
   - Original floating-point input data
   - Format: float32, comma-separated

2. **scaling_factor_per_tensor.txt** (1x4)
   - Single row: original_float, b_int8, c_int8, approximated_float
   - Per-tensor symmetric quantization scaling factor
   - Dyadic approximation: original â‰ˆ b/2^c
   - b and c are int8 values

3. **weight_int8.txt** (192x192)
   - Quantized weights in int8 format
   - Range: [-127, 127]

4. **bias_int32.txt** (1x192)
   - Quantized bias in int32 format

5. **input_quantized_int8.txt** (198x192)
   - Quantized input in int8 format using per-tensor scaling
   - Range: [-127, 127]

6. **linear_output_int32.txt** (198x192)
   - Result of linear operation: input @ weight^T + bias
   - Format: int32

7. **residual_int32.txt** (198x192)
   - Residual data in int32 format
   - Simulates 16-bit range stored in int32

8. **final_output_int32.txt** (198x192)
   - Final result: linear_output + residual
   - Format: int32

## Quantization Details:
- **Activation**: Per-tensor symmetric quantization
- **Input scaling**: Single scaling factor for entire tensor
- **Formula**: quantized_value = round(float_value / scaling_factor)
- **Range**: [-127, 127] for int8

## Hardware Testbench Usage:
Load these files in your testbench to verify the quantized inference implementation.
"""
    
    with open('README.md', 'w') as f:
        f.write(readme_content)
    print("âœ“ Saved README.md")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("Running quantized inference with residual connection...")
    import torch

    checkpoint = torch.load("checkpoint.pth.tar", map_location='cpu')
    # ä¸ä½¿ç”¨ checkpointï¼Œç”Ÿæˆæ¨¡æ“¬æ•¸æ“š
    results = quantized_inference_with_residual(checkpoint=checkpoint)

    print("\n" + "="*80)
    print("âœ… Quantized inference with residual completed successfully!")
    print("ğŸ“ All test data saved to TXT files for hardware testbench.")
    print("ğŸ“‹ Check README.md for detailed file descriptions.")
    
    # é¡¯ç¤ºä¸€äº›çµ±è¨ˆä¿¡æ¯
    print(f"\nğŸ“Š Data Statistics:")
    print(f"   â€¢ Input range: [{results['input_fp32'].min():.6f}, {results['input_fp32'].max():.6f}]")
    print(f"   â€¢ Quantized input range: [{results['input_int8'].min()}, {results['input_int8'].max()}]")
    print(f"   â€¢ Linear output range: [{results['linear_output_int32'].min()}, {results['linear_output_int32'].max()}]")
    print(f"   â€¢ Residual range: [{results['residual_int32'].min()}, {results['residual_int32'].max()}]")
    print(f"   â€¢ Final output range: [{results['final_output_int32'].min()}, {results['final_output_int32'].max()}]")